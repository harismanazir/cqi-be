{
  "chunks": [
    {
      "content": "import requests",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 1,
      "end_line": 1,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755976,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9776343361612528,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "8e41e132c0014809fc7c6d99010a66d6"
      }
    },
    {
      "content": "from bs4 import BeautifulSoup",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 2,
      "end_line": 2,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755979,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.977634336161253,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "c5e6234fcf03913ba1832017d89e8a99"
      }
    },
    {
      "content": "from readability import Document",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 3,
      "end_line": 3,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755979,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.977634336161253,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "620586226bfe74e042d3cfde8eb93ad0"
      }
    },
    {
      "content": "from markdownify import markdownify as md",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 4,
      "end_line": 4,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755979,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.977634336161253,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238077,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "ca6479e1a443c016cd641b15832e9e3c"
      }
    },
    {
      "content": "import pdfplumber",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 5,
      "end_line": 5,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755976,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9776343361612528,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "52ee7484c457eaccdcb2c27f9247226c"
      }
    },
    {
      "content": "import fitz  # PyMuPDF",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 6,
      "end_line": 6,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755976,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9776343361612528,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "dd91a01cf2a1e2ceaba4801fcef12fd4"
      }
    },
    {
      "content": "import os",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 7,
      "end_line": 7,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15099294344755976,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.977634336161253,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.06547096423238076,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "ed9f4b8f879ddbb59fda1057ea3a2810"
      }
    },
    {
      "content": "from typing import List, Dict\n\n",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 8,
      "end_line": 10,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.08849025275771842,
        0.0,
        -0.038369622056060045,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.038369622056060045,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.038369622056060045,
        0.0,
        0.5729480301281699,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.038369622056060045,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.5729480301281699,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.038369622056060045,
        0.5729480301281699,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "6c6a901b13f5d10a0223c9ed11171c8a"
      }
    },
    {
      "content": "def is_substack(url: str) -> bool:\n    return 'substack.com' in url\n\n",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 11,
      "end_line": 14,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.10796870289626538,
        0.0,
        -0.04681553272714996,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.04681553272714996,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.04681553272714996,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.04681553272714996,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.6990651931945489,
        0.0,
        0.0,
        0.0,
        0.0,
        0.6990651931945489,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.04681553272714996,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "5fbd5b4bd001cfb6576aa6c403865282"
      }
    },
    {
      "content": "def scrape_substack(url: str) -> List[Dict]:\n    items = []\n    # Try to get the archive page for all posts\n    archive_url = url.rstrip('/') + '/archive'\n    try:\n        resp = requests.get(archive_url, timeout=10)\n        soup = BeautifulSoup(resp.text, 'html.parser')\n        # Substack archive: all post links are <a data-testid=\"post-preview-title-link\" href=\"/p/...\">\n        post_links = set()\n        for a in soup.find_all('a', attrs={'data-testid': 'post-preview-title-link'}, href=True):\n            href = a['href']",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 15,
      "end_line": 25,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.023512721200614407,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.023512721200614407,
        0.0,
        0.0,
        0.0,
        0.35109981732846396,
        0.35109981732846396,
        0.0,
        0.0,
        0.0,
        0.21306154130886742,
        0.0,
        0.0,
        -0.023512721200614407,
        0.46066696099819904,
        0.35109981732846396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.35109981732846396,
        -0.023512721200614407,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.35109981732846396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.35109981732846396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.023512721200614407,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "139871d9f4943cf7cb21cf052aefd948"
      }
    },
    {
      "content": "            if href.startswith('/p/'):\n                post_links.add(requests.compat.urljoin(url, href))\n        for link in post_links:\n            try:\n                post_resp = requests.get(link, timeout=10)\n                post_soup = BeautifulSoup(post_resp.text, 'html.parser')\n                # Title: <h1>\n                title_tag = post_soup.find('h1')\n                title = title_tag.get_text(strip=True) if title_tag else link\n                # Author: <a rel=\"author\"> or meta\n                author = ''",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 26,
      "end_line": 36,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027942615940422473,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027942615940422473,
        0.0,
        0.0,
        0.0,
        0.4172484872616698,
        0.4172484872616698,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027942615940422473,
        0.5474585377756229,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027942615940422473,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.4172484872616698,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.4172484872616698,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027942615940422473,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "61388c4df42defce0d86005cfb909c6b"
      }
    },
    {
      "content": "                author_tag = post_soup.find('a', rel='author')\n                if author_tag:\n                    author = author_tag.get_text(strip=True)\n                else:\n                    meta_author = post_soup.find('meta', attrs={'name': 'author'})\n                    if meta_author:\n                        author = meta_author.get('content', '')\n                # Main content: <div class=\"body\"> or <article>\n                content_div = post_soup.find('div', class_='body')\n                if not content_div:",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 37,
      "end_line": 46,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.029618854533398818,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.029618854533398818,
        0.0,
        0.0,
        0.0,
        0.44227864258786564,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.029618854533398818,
        0.5802998125879203,
        0.0,
        0.0,
        0.0,
        0.0,
        0.2683925329121066,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.029618854533398818,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.44227864258786564,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.44227864258786564,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.029618854533398818,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "16d85c82049e6000cae8e0c953f185ab"
      }
    },
    {
      "content": "                    content_div = post_soup.find('article')\n                content_html = str(content_div) if content_div else ''\n                content_md = md(content_html)\n                items.append({\n                    \"title\": title,\n                    \"content\": content_md,\n                    \"content_type\": \"blog\",\n                    \"source_url\": link,\n                    \"author\": author,\n                    \"user_id\": \"\"\n                })\n            except Exception:\n                continue",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 47,
      "end_line": 59,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.033024407510730464,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.033024407510730464,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.2992520986836908,
        0.0,
        0.0,
        -0.033024407510730464,
        0.6470222360454291,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.033024407510730464,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.4931314987095255,
        0.0,
        0.0,
        0.0,
        0.0,
        0.4931314987095255,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.033024407510730464,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "70461f4239445d7b45017e54c7cafb7b"
      }
    },
    {
      "content": "    except Exception:\n        pass\n    return items\n\n",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 60,
      "end_line": 64,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.1298594690747009,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.5102322188555724,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8408013841202439,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "5b3160cf2bd2482a8fcdf90b658b8d36"
      }
    },
    {
      "content": "def extract_blog_links(soup, base_url):\n    links = set()\n    for a in soup.find_all('a', href=True):\n        href = a['href']\n        # Accept any /blog/ subpath that isn't just /blog/\n        if href.startswith('/blog/') and href != '/blog/':\n            full_url = requests.compat.urljoin(base_url, href)\n            links.add(full_url)\n    return links\n\n\nasync def scrape_blogs(urls: List[str]) -> List[Dict]:\n    items = []\n    for url in urls:\n        try:\n            if is_substack(url):\n                items.extend(scrape_substack(url))",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 65,
      "end_line": 81,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.017777038625788028,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.1610874052901497,
        0.0,
        -0.017777038625788028,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.3221748105802994,
        0.0,
        0.0,
        -0.017777038625788028,
        0.3482920708928994,
        0.5309053733850362,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.017777038625788028,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.20667720823374627,
        0.0,
        0.2654526866925181,
        0.0,
        0.0,
        0.0,
        0.0,
        0.2654526866925181,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.017777038625788028,
        0.0,
        0.0,
        0.5309053733850362,
        0.0
      ],
      "metadata": {
        "hash": "59cfd74d599b0e046b801e847a8dd505"
      }
    },
    {
      "content": "                continue\n            resp = requests.get(url, timeout=10)\n            soup = BeautifulSoup(resp.text, 'html.parser')\n            # Use improved extraction for /blog/ links\n            links = extract_blog_links(soup, url)\n            if not links:\n                links = {url}\n            for link in links:\n                try:\n                    post_resp = requests.get(link, timeout=10)\n                    doc = Document(post_resp.text)\n                    title = doc.title() or link",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 82,
      "end_line": 93,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.024065289584607807,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.024065289584607807,
        0.0,
        0.0,
        0.0,
        0.7187019065144701,
        0.35935095325723504,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.024065289584607807,
        0.4714930153721723,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.024065289584607807,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.35935095325723504,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.024065289584607807,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "5e544a52e92d0f9df9e5bd494f38cf2c"
      }
    },
    {
      "content": "                    content_html = doc.summary()\n                    content_md = md(content_html)\n                    items.append({\n                        \"title\": title,\n                        \"content\": content_md,\n                        \"content_type\": \"blog\",\n                        \"source_url\": link,\n                        \"author\": \"\",\n                        \"user_id\": \"\"\n                    })\n                except Exception:\n                    continue\n        except Exception:\n            continue",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 94,
      "end_line": 107,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03796106205151378,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03796106205151378,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.34398580757235947,
        0.0,
        0.0,
        -0.03796106205151378,
        0.7437423742802661,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03796106205151378,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.5668472754881598,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03796106205151378,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "1a27661e41e7cb05dd4f356fcaf4ebd7"
      }
    },
    {
      "content": "    return items\n\n",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 108,
      "end_line": 110,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.12985946907470092,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.5102322188555724,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.840801384120244,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.05630743040636514,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "3cc25be1764a097634e44d98f9c6d842"
      }
    },
    {
      "content": "def scrape_pdf(pdf_path: str) -> List[Dict]:\n    items = []\n    try:\n        doc = fitz.open(pdf_path)\n        num_pages = doc.page_count\n        chunk_size = max(1, num_pages // 8)\n        for i in range(0, min(num_pages, 8 * chunk_size), chunk_size):\n            text = \"\"\n            for j in range(i, min(i + chunk_size, num_pages)):\n                page = doc.load_page(j)\n                text += page.get_text(\"text\") or \"\"\n            # If text is too short (likely only headings), fallback to pdfplumber",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 111,
      "end_line": 122,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027691222560455316,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027691222560455316,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.25092521231882436,
        0.0,
        0.0,
        -0.027691222560455316,
        0.5425331774408307,
        0.41349459722780396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.25092521231882436,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027691222560455316,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.25092521231882436,
        0.41349459722780396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.41349459722780396,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.027691222560455316,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "3608f4baf38dc9415c3f2f1ffbdedba4"
      }
    },
    {
      "content": "            if len(text.strip()) < 1000:\n                with pdfplumber.open(pdf_path) as pdf:\n                    text = \"\"\n                    for j in range(i, min(i + chunk_size, num_pages)):\n                        page = pdf.pages[j]\n                        page_text = page.extract_text() or \"\"\n                        if not page_text or len(page_text.strip()) < 100:\n                            # Try to concatenate all text objects\n                            blocks = page.extract_words() or []",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 123,
      "end_line": 131,
      "language": "python",
      "chunk_type": "large_block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.022855803071883663,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.022855803071883663,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.022855803071883663,
        0.4477964610077892,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.207108849239459,
        0.0,
        0.0,
        0.0,
        0.414217698478918,
        0.0,
        -0.022855803071883663,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.34129049610914486,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.6825809922182897,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.022855803071883663,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "abe161ae42e31f924ebe1bc27bdabed9"
      }
    },
    {
      "content": "                            page_text = ' '.join([b['text'] for b in blocks])\n                        text += page_text + \"\\n\"\n            items.append({\n                \"title\": f\"Book Chapter {i // chunk_size + 1}\",\n                \"content\": text.strip(),\n                \"content_type\": \"book\",\n                \"source_url\": \"\",\n                \"author\": \"\",\n                \"user_id\": \"\"\n            })\n    except Exception:\n        pass\n    return items ",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 132,
      "end_line": 144,
      "language": "python",
      "chunk_type": "block",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.08232886442475103,
        0.0,
        -0.03569802677512153,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03569802677512153,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.6469584308424382,
        0.0,
        0.0,
        -0.03569802677512153,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03569802677512153,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.533054875817788,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.533054875817788,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.03569802677512153,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "b75ddd75f8468b5c1eaf52d675ef5b04"
      }
    },
    {
      "content": "import requests\nfrom bs4 import BeautifulSoup\nfrom readability import Document\nfrom markdownify import markdownify as md\nimport pdfplumber\nimport fitz  # PyMuPDF\nimport os\nfrom typing import List, Dict\n\n\ndef is_substack(url: str) -> bool:\n    return 'substack.com' in url\n\n\ndef scrape_substack(url: str) -> List[Dict]:\n    items = []\n    # Try to get the archive page for all posts\n    archive_url = url.rstrip('/') + '/archive'\n    try:\n        resp = requests.get(archive_url, timeout=10)\n        soup = BeautifulSoup(resp.text, 'html.parser')\n        # Substack archive: all post links are <a data-testid=\"post-preview-title-link\" href=\"/p/...\">\n        post_links = set()\n        for a in soup.find_all('a', attrs={'data-testid': 'post-preview-title-link'}, href=True):\n            href = a['href']\n            if href.startswith('/p/'):\n                post_links.add(requests.compat.urljoin(url, href))\n        for link in post_links:\n            try:\n                post_resp = requests.get(link, timeout=10)\n                post_soup = BeautifulSoup(post_resp.text, 'html.parser')\n                # Title: <h1>\n                title_tag = post_soup.find('h1')\n                title = title_tag.get_text(strip=True) if title_tag else link\n                # Author: <a rel=\"author\"> or meta\n                author = ''\n                author_tag = post_soup.find('a', rel='author')\n                if author_tag:\n                    author = author_tag.get_text(strip=True)\n                else:\n                    meta_author = post_soup.find('meta', attrs={'name': 'author'})\n                    if meta_author:\n                        author = meta_author.get('content', '')\n                # Main content: <div class=\"body\"> or <article>\n                content_div = post_soup.find('div', class_='body')\n                if not content_div:\n                    content_div = post_soup.find('article')\n                content_html = str(content_div) if content_div else ''\n                content_md = md(content_html)\n                items.append({",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 1,
      "end_line": 50,
      "language": "python",
      "chunk_type": "sliding_window",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.011182650272777943,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.011182650272777943,
        0.0,
        0.0,
        0.0,
        0.500949222489548,
        0.33396614832636534,
        0.0,
        0.0,
        0.0,
        0.2026641393573651,
        0.0,
        0.0,
        -0.011182650272777943,
        0.0,
        0.33396614832636534,
        0.0,
        0.0,
        0.0,
        0.10133206967868255,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.16698307416318267,
        -0.011182650272777943,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.16698307416318267,
        0.0,
        0.16698307416318267,
        0.0,
        0.0,
        0.500949222489548,
        0.33396614832636534,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.011182650272777943,
        0.16698307416318267,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "f16aa1bc975a3ccdd031c6e8b2128343"
      }
    },
    {
      "content": "                    meta_author = post_soup.find('meta', attrs={'name': 'author'})\n                    if meta_author:\n                        author = meta_author.get('content', '')\n                # Main content: <div class=\"body\"> or <article>\n                content_div = post_soup.find('div', class_='body')\n                if not content_div:\n                    content_div = post_soup.find('article')\n                content_html = str(content_div) if content_div else ''\n                content_md = md(content_html)\n                items.append({\n                    \"title\": title,\n                    \"content\": content_md,\n                    \"content_type\": \"blog\",\n                    \"source_url\": link,\n                    \"author\": author,\n                    \"user_id\": \"\"\n                })\n            except Exception:\n                continue\n    except Exception:\n        pass\n    return items\n\n\ndef extract_blog_links(soup, base_url):\n    links = set()\n    for a in soup.find_all('a', href=True):\n        href = a['href']\n        # Accept any /blog/ subpath that isn't just /blog/\n        if href.startswith('/blog/') and href != '/blog/':\n            full_url = requests.compat.urljoin(base_url, href)\n            links.add(full_url)\n    return links\n\n\nasync def scrape_blogs(urls: List[str]) -> List[Dict]:\n    items = []\n    for url in urls:\n        try:\n            if is_substack(url):\n                items.extend(scrape_substack(url))\n                continue\n            resp = requests.get(url, timeout=10)\n            soup = BeautifulSoup(resp.text, 'html.parser')\n            # Use improved extraction for /blog/ links\n            links = extract_blog_links(soup, url)\n            if not links:\n                links = {url}\n            for link in links:\n                try:",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 41,
      "end_line": 90,
      "language": "python",
      "chunk_type": "sliding_window",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01306916954514971,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.11842684575546898,
        0.0,
        -0.01306916954514971,
        0.0,
        0.0,
        0.0,
        0.39030642185447123,
        0.19515321092723562,
        0.0,
        0.0,
        0.0,
        0.4737073830218759,
        0.0,
        0.0,
        -0.01306916954514971,
        0.0,
        0.39030642185447123,
        0.0,
        0.0,
        0.0,
        0.11842684575546898,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01306916954514971,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.15194316288466223,
        0.0,
        0.19515321092723562,
        0.0,
        0.19515321092723562,
        0.0,
        0.0,
        0.39030642185447123,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01306916954514971,
        0.0,
        0.0,
        0.39030642185447123,
        0.0
      ],
      "metadata": {
        "hash": "c856095abea8e1153833ef8d730eea9c"
      }
    },
    {
      "content": "                items.extend(scrape_substack(url))\n                continue\n            resp = requests.get(url, timeout=10)\n            soup = BeautifulSoup(resp.text, 'html.parser')\n            # Use improved extraction for /blog/ links\n            links = extract_blog_links(soup, url)\n            if not links:\n                links = {url}\n            for link in links:\n                try:\n                    post_resp = requests.get(link, timeout=10)\n                    doc = Document(post_resp.text)\n                    title = doc.title() or link\n                    content_html = doc.summary()\n                    content_md = md(content_html)\n                    items.append({\n                        \"title\": title,\n                        \"content\": content_md,\n                        \"content_type\": \"blog\",\n                        \"source_url\": link,\n                        \"author\": \"\",\n                        \"user_id\": \"\"\n                    })\n                except Exception:\n                    continue\n        except Exception:\n            continue\n    return items\n\n\ndef scrape_pdf(pdf_path: str) -> List[Dict]:\n    items = []\n    try:\n        doc = fitz.open(pdf_path)\n        num_pages = doc.page_count\n        chunk_size = max(1, num_pages // 8)\n        for i in range(0, min(num_pages, 8 * chunk_size), chunk_size):\n            text = \"\"\n            for j in range(i, min(i + chunk_size, num_pages)):\n                page = doc.load_page(j)\n                text += page.get_text(\"text\") or \"\"\n            # If text is too short (likely only headings), fallback to pdfplumber\n            if len(text.strip()) < 1000:\n                with pdfplumber.open(pdf_path) as pdf:\n                    text = \"\"\n                    for j in range(i, min(i + chunk_size, num_pages)):\n                        page = pdf.pages[j]\n                        page_text = page.extract_text() or \"\"\n                        if not page_text or len(page_text.strip()) < 100:\n                            # Try to concatenate all text objects",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 81,
      "end_line": 130,
      "language": "python",
      "chunk_type": "sliding_window",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.014087901977802766,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.12765813380717778,
        0.0,
        -0.014087901977802766,
        0.0,
        0.0,
        0.0,
        0.420730528699384,
        0.210365264349692,
        0.0,
        0.0,
        0.0,
        0.5106325352287111,
        0.0,
        0.0,
        -0.014087901977802766,
        0.0,
        0.210365264349692,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.25531626761435555,
        0.0,
        0.0,
        0.0,
        0.25531626761435555,
        0.0,
        -0.014087901977802766,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.12765813380717778,
        0.210365264349692,
        0.0,
        0.210365264349692,
        0.0,
        0.0,
        0.210365264349692,
        0.420730528699384,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.014087901977802766,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "931e7a3b3a59bd6261f07f50499a449c"
      }
    },
    {
      "content": "                text += page.get_text(\"text\") or \"\"\n            # If text is too short (likely only headings), fallback to pdfplumber\n            if len(text.strip()) < 1000:\n                with pdfplumber.open(pdf_path) as pdf:\n                    text = \"\"\n                    for j in range(i, min(i + chunk_size, num_pages)):\n                        page = pdf.pages[j]\n                        page_text = page.extract_text() or \"\"\n                        if not page_text or len(page_text.strip()) < 100:\n                            # Try to concatenate all text objects\n                            blocks = page.extract_words() or []\n                            page_text = ' '.join([b['text'] for b in blocks])\n                        text += page_text + \"\\n\"\n            items.append({\n                \"title\": f\"Book Chapter {i // chunk_size + 1}\",\n                \"content\": text.strip(),\n                \"content_type\": \"book\",\n                \"source_url\": \"\",\n                \"author\": \"\",\n                \"user_id\": \"\"\n            })\n    except Exception:\n        pass\n    return items ",
      "file_path": "C:\\Users\\haris\\AppData\\Local\\Temp\\cqi_github_-aline-scraper_70em11_d\\scraper.py",
      "start_line": 121,
      "end_line": 144,
      "language": "python",
      "chunk_type": "sliding_window",
      "embedding": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01768672290573684,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01768672290573684,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.32053801096409645,
        0.0,
        0.0,
        -0.01768672290573684,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.16026900548204823,
        0.0,
        0.0,
        0.0,
        0.32053801096409645,
        0.0,
        -0.01768672290573684,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.2641040621525805,
        0.0,
        0.2641040621525805,
        0.0,
        0.0,
        0.0,
        0.7923121864577415,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        -0.01768672290573684,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "metadata": {
        "hash": "4de30bd7a151957a034dc54bc2ba74a1"
      }
    }
  ],
  "embedding_method": "tfidf",
  "version": "1.0"
}